import asyncio
import base64
import io

import streamlit as st
from openai import OpenAI
from PIL import Image
from rsstvlm.agent.workflow import AgentWorkflow, StreamEvent
from rsstvlm.utils import (
    LLM_MODEL,
    QWEN3_VL_30B_API_BASE,
    qwen3_vl_30b_function,
)

client = OpenAI(api_key="EMPTY", base_url=QWEN3_VL_30B_API_BASE)

st.set_page_config(
    page_title="å¤©ç©ºåœ°ä¸€ä½“åŒ–è¶…å…‰è°±é¥æ„Ÿåº”ç”¨å·¥ç¨‹å®éªŒå®¤",
    page_icon="ğŸ›°ï¸",
    layout="wide",
)


# ======================
# ğŸ¤– Agent åˆå§‹åŒ– (cached)
# ======================
@st.cache_resource
def get_agent():
    """Initialize agent once and cache it."""
    return asyncio.run(
        AgentWorkflow.create(qwen3_vl_30b_function, timeout=120, verbose=True)
    )


# ======================
# ğŸ§© ä¾§è¾¹æ åŠŸèƒ½
# ======================
with st.sidebar:
    st.header("ğŸ”§ æ§åˆ¶é¢æ¿")

    if st.button("ğŸ—‘ï¸ æ¸…ç©ºå½“å‰å¯¹è¯"):
        st.session_state.messages = []
        st.rerun()

    st.divider()

    # Agent mode toggle
    use_agent = st.toggle("ğŸ¤– å¯ç”¨ Agent æ¨¡å¼", value=False)

    if use_agent:
        with st.spinner("æ­£åœ¨åŠ è½½ Agent..."):
            try:
                agent = get_agent()
                st.success("Agent å·²å°±ç»ª âœ…")
                # æ˜¾ç¤ºå¯ç”¨å·¥å…·
                st.markdown("#### ğŸ› ï¸ å¯ç”¨å·¥å…·")
                for tool in agent.tools:
                    with st.expander(f"ğŸ“¦ {tool.metadata.name}"):
                        st.markdown(f"**æè¿°:** {tool.metadata.description}")
                        if tool.metadata.fn_schema:
                            st.markdown("**å‚æ•°:**")
                            schema = (
                                tool.metadata.fn_schema.model_json_schema()
                            )
                            if "properties" in schema:
                                for param, info in schema[
                                    "properties"
                                ].items():
                                    param_type = info.get("type", "any")
                                    param_desc = info.get("description", "")
                                    st.markdown(
                                        f"- `{param}` ({param_type}): {param_desc}"
                                    )
            except Exception as e:
                st.error(f"Agent åŠ è½½å¤±è´¥: {e}")
                use_agent = False

    st.divider()
    st.markdown("### ğŸ“Œ ä½¿ç”¨è¯´æ˜")
    st.markdown("""
    - å¯ç”¨ Agent å¯ä»¥æŸ¥çœ‹å¹¶ä½¿ç”¨å·¥å…·
    - ç›®å‰å·¥å…·è¾ƒå°‘ï¼Œåç»­ä¼šå®Œå–„
    """)  # noqa: RUF001

# ======================
# ğŸ’¬ èŠå¤©å†å²åˆå§‹åŒ–
# ======================
if "messages" not in st.session_state:
    st.session_state.messages = []

# æ˜¾ç¤ºå†å²æ¶ˆæ¯
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        if isinstance(message["content"], list):
            # å¤šæ¨¡æ€æ¶ˆæ¯
            for part in message["content"]:
                if part["type"] == "text":
                    st.markdown(part["text"])
                elif part["type"] == "image_url":
                    st.image(part["image_url"]["url"], width=300)
        else:
            st.markdown(message["content"])

# ======================
# ğŸ–¼ï¸ å›¾åƒä¸Šä¼  + æ–‡æœ¬è¾“å…¥
# ======================
uploaded_file = st.file_uploader("ğŸ“ ä¸Šä¼ å›¾åƒ/è§†é¢‘")

# æ£€æŸ¥æ˜¯å¦å·²æœ‰å¤ªå¤šæ¶ˆæ¯(é˜²è¿‡è½½)
if len(st.session_state.messages) >= 10:
    st.warning("å¯¹è¯è¾ƒé•¿ï¼Œå»ºè®®ç‚¹å‡»ä¾§è¾¹æ ã€Œæ¸…ç©ºå½“å‰å¯¹è¯ã€ä»¥è·å¾—æœ€ä½³ä½“éªŒã€‚")  # noqa: RUF001

prompt = st.chat_input("è¯·è¾“å…¥ä½ çš„é—®é¢˜")


# ======================
# ğŸ”„ Agent è°ƒç”¨å‡½æ•°
# ======================
async def run_agent_stream(agent: AgentWorkflow, query: str):
    """Run agent and yield streaming events."""
    handler = agent.run(input=query)
    async for event in handler.stream_events():
        if isinstance(event, StreamEvent):
            yield event.delta
    # Ensure handler completes
    await handler


def run_agent(query: str) -> str:
    """Run agent synchronously with streaming output."""
    agent = get_agent()

    async def collect_response():
        full_response = ""
        async for delta in run_agent_stream(agent, query):
            full_response += delta
        return full_response

    return asyncio.run(collect_response())


def run_agent_with_placeholder(query: str, placeholder) -> str:
    """Run agent with live streaming to a placeholder."""
    agent = get_agent()

    async def stream_to_placeholder():
        full_response = ""
        handler = agent.run(input=query)
        async for event in handler.stream_events():
            if isinstance(event, StreamEvent):
                full_response += event.delta
                placeholder.markdown(full_response + "â–Œ")
        await handler
        return full_response

    return asyncio.run(stream_to_placeholder())


# ======================
# ğŸ¤– å¤„ç†ç”¨æˆ·è¾“å…¥
# ======================
if prompt:
    # æ„å»ºç”¨æˆ·å†…å®¹
    user_content = [{"type": "text", "text": prompt}]

    # å¦‚æœä¸Šä¼ äº†å›¾ç‰‡
    if uploaded_file is not None:
        image = Image.open(uploaded_file).convert("RGB")
        buffered = io.BytesIO()
        image.save(buffered, format="JPEG")
        img_str = base64.b64encode(buffered.getvalue()).decode()
        image_url = f"data:image/jpeg;base64,{img_str}"
        user_content.insert(
            0, {"type": "image_url", "image_url": {"url": image_url}}
        )

    # æ˜¾ç¤ºç”¨æˆ·æ¶ˆæ¯
    with st.chat_message("user"):
        if uploaded_file is not None:
            st.image(image, width=300)
        st.markdown(prompt)

    # æ·»åŠ åˆ°å†å²
    st.session_state.messages.append({"role": "user", "content": user_content})

    # ======================
    # ğŸ§  è°ƒç”¨å¤§æ¨¡å‹æˆ–Agent
    # ======================
    with st.chat_message("assistant"):
        message_placeholder = st.empty()
        full_response = ""

        try:
            if use_agent:
                # Use Agent with streaming
                full_response = run_agent_with_placeholder(
                    prompt, message_placeholder
                )
                message_placeholder.markdown(full_response)
            else:
                # Use direct LLM call
                # TODO: will be deprecated while agent 1st demo is released
                response = client.chat.completions.create(
                    model=LLM_MODEL,
                    messages=[
                        {"role": m["role"], "content": m["content"]}
                        for m in st.session_state.messages
                    ],
                    stream=True,
                )

                for chunk in response:
                    delta = chunk.choices[0].delta.content or ""
                    full_response += delta
                    message_placeholder.markdown(full_response + "â–Œ")

                message_placeholder.markdown(full_response)

        except Exception as e:
            error_msg = f"âŒ æ¨¡å‹è°ƒç”¨å¤±è´¥: {e!s}"
            message_placeholder.error(error_msg)
            full_response = error_msg

    # æ·»åŠ åŠ©æ‰‹å›å¤
    st.session_state.messages.append(
        {"role": "assistant", "content": full_response}
    )
